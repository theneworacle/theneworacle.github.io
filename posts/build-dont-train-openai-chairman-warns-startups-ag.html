<!DOCTYPE html><html><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><link rel="preload" href="/_next/static/css/0f953e58cdaad9b8.css" as="style"/><link rel="stylesheet" href="/_next/static/css/0f953e58cdaad9b8.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" noModule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-cfb97e7cd53c187e.js" defer=""></script><script src="/_next/static/chunks/framework-2f335d22a7318891.js" defer=""></script><script src="/_next/static/chunks/main-dbd0de0e54d73d4c.js" defer=""></script><script src="/_next/static/chunks/pages/_app-e000f3c4926d5b9b.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bslug%5D-d2f93a7f1d7337ac.js" defer=""></script><script src="/_next/static/BYAS7YnIXhi8qMX8suLyx/_buildManifest.js" defer=""></script><script src="/_next/static/BYAS7YnIXhi8qMX8suLyx/_ssgManifest.js" defer=""></script></head><body><div id="__next"></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"postData":{"id":"build-dont-train-openai-chairman-warns-startups-ag","contentHtml":"\u003ch3\u003eThe High Cost of Ambition\u003c/h3\u003e\n\u003cp\u003eIn a candid conversation on the \u003cem\u003eMinus One\u003c/em\u003e podcast, OpenAI chairman Bret Taylor delivered a sobering message to the AI community, likening the decision to train a new frontier AI model to the Joker burning a mountain of cash in \"The Dark Knight.\" Having been involved with LLM training at Google, Facebook, and OpenAI, Taylor's advice is clear: building a foundational model from scratch is a \"good way to burn through millions of dollars.\"\u003c/p\u003e\n\u003cp\u003eTaylor argues that the sheer amount of capital required to train a competitive large language model (LLM) is prohibitive for almost everyone. \"Unless you work at OpenAI or Anthropic or Google or Meta, you're probably not building one of those,\" he stated. \"It requires so much capital that it will tend towards consolidation.\" This high barrier to entry, he says, prevents an \"indie data center market\" from emerging.\u003c/p\u003e\n\u003cp\u003eHis recommendation for founders? Leverage the work already done by the AI juggernauts. Instead of training, build services and applications that use the APIs of established models. While this advice directly benefits companies like OpenAI, which sells access to its models, Taylor suggests it's the most practical path forward. He points to two promising avenues for entrepreneurs: the \"AI tools market\"—the proverbial \"pickaxes in the gold rush\"—and building \"applied AI companies,\" which he predicts will be the successors to today's SaaS businesses.\u003c/p\u003e\n\u003ch3\u003eA Challenger Emerges\u003c/h3\u003e\n\u003cp\u003eHowever, Taylor's theory of inevitable consolidation is not without its challengers. The American LLM market may be consolidating, but international players are proving that a different path is possible. Chinese AI startup DeepSeek, for instance, successfully launched its R1 reasoning model and a companion chatbot that soared to the top of the app store charts, even surpassing ChatGPT.\u003c/p\u003e\n\u003cp\u003eCrucially, DeepSeek achieved this success by using fewer, less advanced chips, significantly minimizing their capital costs. This has ignited a debate on Wall Street and in the tech world about whether the American tech giants are overspending on model development and if a more efficient approach is being overlooked.\u003c/p\u003e\n\u003ch3\u003eThe Path Forward: Consolidate or Innovate?\u003c/h3\u003e\n\u003cp\u003eThe debate highlights a central tension in the AI industry. On one hand, the cost and complexity of building cutting-edge models are immense, creating what Taylor calls \"fast-depreciating assets.\" His advice to lease or use open-source models rather than building your own seems a pragmatic, capital-preserving strategy.\u003c/p\u003e\n\u003cp\u003eOn the other hand, the success of companies like DeepSeek demonstrates that innovation isn't exclusive to those with the deepest pockets. It suggests that alternative, more cost-effective methods can yield powerful results, countering the narrative that power will inevitably centralize in the hands of a few tech behemoths. For the broader AI ecosystem, the question remains: is the future one of building on the foundations laid by giants, or will new methods continue to emerge, leveling the playing field for all?\u003c/p\u003e\n","title":"Build, Don't Train: OpenAI Chairman Warns Startups Against Burning Capital on New AI Models","authors":[{"username":"@alanaturner","name":"Alana Turner"}],"date":"2025-07-26T08:38:36Z","summary":"OpenAI's chairman, Bret Taylor, has a stark warning for AI entrepreneurs: training your own large language model is a surefire way to \"destroy your capital.\" He argues that the astronomical costs are creating a market dominated by a few tech giants, and advises startups to instead focus on building applications on top of existing platforms. However, the rise of cost-effective international models challenges this view.","tags":["AI","OpenAI","Large Language Models","Startups","Bret Taylor","DeepSeek","AI Development","Venture Capital"],"sources":[{"url":"https://www.businessinsider.com/openai-chairman-bret-taylor-ai-model-costs-destroy-capital-2025-7","title":"OpenAI chairman says training your own AI model is a good way to 'destroy your capital'"}]}},"__N_SSG":true},"page":"/posts/[slug]","query":{"slug":"build-dont-train-openai-chairman-warns-startups-ag"},"buildId":"BYAS7YnIXhi8qMX8suLyx","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>